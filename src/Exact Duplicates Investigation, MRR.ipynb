{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2>Exact Duplicates Investigation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tactics_data = pd.read_csv(\"modified tactics data\")\n",
    "tactics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tactics_data = pd.read_csv('/w/225/1/chess/tactics/tactics_problem.csv')\n",
    "#tactics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to extract ONLY the field indicated by parameter if it exists. Otherwise, it's skipped\n",
    "def extract(st, parameter):\n",
    "    splitted_st = st.split(\"\\\\n\") #splitted strings\n",
    "    existence = 0\n",
    "    for string in splitted_st:\n",
    "        if parameter in string: #if we find the section which has parameter in it (as the indicator)\n",
    "            target_st = string #take it as our target string\n",
    "            existence = 1 #to denote that the parameter field indeed exists\n",
    "            break\n",
    "    if existence == 1: #if the parameter field exists, clean it up so that we get only the relevant information. Otherwise, skip\n",
    "        target_st = target_st.replace(parameter, '')\n",
    "        target_st = target_st.replace('[','')\n",
    "        target_st = target_st.replace(']','')\n",
    "        target_st = target_st.replace('\"','')\n",
    "        return target_st #return the cleaned target string if the parameter field exists\n",
    "    return \"N/A\" #return N/A if the parameter field does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a FULL column, append it to the tactics_data. Repeat this same process for FEN, Event, and so on.\n",
    "\n",
    "\n",
    "tactics_data['FULL'] = 'N/A'\n",
    "tactics_data['FEN'] = 'N/A'\n",
    "tactics_data['Event'] = 'N/A'\n",
    "tactics_data['Site'] = 'N/A'\n",
    "tactics_data['Date'] = 'N/A'\n",
    "tactics_data['Round'] = 'N/A'\n",
    "tactics_data['White'] = 'N/A'\n",
    "tactics_data['Black'] = 'N/A'\n",
    "tactics_data['Result'] = 'N/A'\n",
    "tactics_data['FirstMove'] = 'N/A'\n",
    "tactics_data['PlyCount'] = 'N/A'\n",
    "tactics_data['SetUp'] = 'N/A' \n",
    "\n",
    "row, col = tactics_data.shape\n",
    "puzz_id = list(tactics_data['tactics_problem_id'])\n",
    "for i in tqdm(range(row)):\n",
    "    pos = puzz_id.index(tactics_data.iloc[i,0])\n",
    "    pgn = tactics_data.iloc[pos,7]\n",
    "    tactics_data.iloc[i,8] = extract(pgn, \"FULL\")\n",
    "    tactics_data.iloc[i,9] = extract(pgn, \"FEN\")\n",
    "    tactics_data.iloc[i,10] = extract(pgn, \"Event\")\n",
    "    tactics_data.iloc[i,11] = extract(pgn, \"Site\")\n",
    "    tactics_data.iloc[i,12] = extract(pgn, \"Date\")\n",
    "    tactics_data.iloc[i,13] = extract(pgn, \"Round\")\n",
    "    tactics_data.iloc[i,14] = extract(pgn, \"White\")\n",
    "    tactics_data.iloc[i,15] = extract(pgn, \"Black\")\n",
    "    tactics_data.iloc[i,16] = extract(pgn, \"Result\")\n",
    "    tactics_data.iloc[i,17] = extract(pgn, \"FirstMove\")\n",
    "    tactics_data.iloc[i,18] = extract(pgn, \"PlyCount\")\n",
    "    tactics_data.iloc[i,19] = extract(pgn, \"SetUp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A list of ALL the cleaned FEN and cleaned FULL fields in the form of (FEN, FULL) tuple, EXCLUDING N/A's.\n",
    "\n",
    "full_list_ori = np.array(tactics_data['FULL'])\n",
    "fen_list_ori = np.array(tactics_data['FEN'])\n",
    "fen_full_list_cleaned = []\n",
    "for i in range(len(full_list_ori)):\n",
    "    if full_list_ori[i] != 'N/A' and fen_list_ori[i] != 'N/A':\n",
    "        fen_full_list_cleaned.append((fen_list_ori[i], full_list_ori[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fen_full_to_ids is a dictionary that maps (FEN, FULL) tuple to ALL OF ITS CORRESPONDING PROBLEM IDs,\n",
    "#which are colleted in a list\n",
    "\n",
    "fen_full_to_ids = {}\n",
    "for fen_full in fen_full_list_cleaned:\n",
    "    fen_full_to_ids[fen_full] = []\n",
    "    \n",
    "r,c = tactics_data.shape\n",
    "for i in tqdm(range(r)):\n",
    "    fen = tactics_data.iloc[i,9]\n",
    "    full = tactics_data.iloc[i,8]\n",
    "    prob_id = tactics_data.iloc[i,0]\n",
    "    if full != \"N/A\" and fen != \"N/A\":\n",
    "        fen_full_to_ids[(fen,full)].append(prob_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dup_fen_full_to_ids is a subset of fen_full_to_ids. Here, only ids which have at least 1 duplicate are selected.\n",
    "dup_fen_full_to_ids = {}\n",
    "\n",
    "for pair in fen_full_to_ids:\n",
    "    if len(fen_full_to_ids[pair]) > 1:\n",
    "        dup_fen_full_to_ids[pair] = fen_full_to_ids[pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_fen_full_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate_fen_full_id_to_id is a list that pairs an ID to another (FEN,FULL) duplicate ID in the form of tuple. If there are more than two\n",
    "#problems which are duplicate to each other, we form every possible 2-way pair of all the IDs.\n",
    "duplicate_fen_full_id_to_id = []\n",
    "#target_problems is a list of all the problems which have at least one duplicate and so \n",
    "#we have to assess for their (reciprocal) rank\n",
    "target_problems = []\n",
    "#dup_fen_full_list is a list that pairs FEN and FULL in the form of (FEN, FULL) tuple, but ONLY those which have duplicates.\n",
    "dup_fen_full_list = []\n",
    "#group is a list to denote the \"group\" of each problem id. Here, group denotes which ones are duplicate with each other\n",
    "#Ex:\n",
    "#752 -> group 1\n",
    "#765 -> group 1\n",
    "#This means 752 and 765 are exact duplicates (exactly same FEN and FULL) with each other\n",
    "group = []\n",
    "#ind is just index for the group list.\n",
    "ind = 0\n",
    "\n",
    "for fen_full in dup_fen_full_to_ids: #for every (FEN,FULL) field\n",
    "    prob_id_list = dup_fen_full_to_ids[fen_full] #get the list of corresponding problem IDs\n",
    "    target_problems += prob_id_list #add that ID to the target_problems list\n",
    "    ind += 1\n",
    "    #create all pairs of IDs in the list\n",
    "    for k in range(len(prob_id_list)):\n",
    "        group.append(ind)\n",
    "        dup_fen_full_list.append(fen_full)\n",
    "        for j in range(len(prob_id_list)):\n",
    "            if prob_id_list[k] != prob_id_list[j]:\n",
    "                duplicate_fen_full_id_to_id.append((prob_id_list[k],prob_id_list[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_fen_full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_fen_full_pair = tactics_data.set_index(\"tactics_problem_id\", inplace = False)\n",
    "new_table_fen_full_pair = new_table_fen_full_pair.loc[target_problems]\n",
    "new_table_fen_full_pair.insert(0, \"(FEN,FULL)\", dup_fen_full_list)\n",
    "new_table_fen_full_pair.insert(0, \"Group\", group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_fen_full_pair.to_csv(\"new_tactics_problem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"new_tactics_problem.csv\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Summary Statistics of Dates for Each Group </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_date = temp[['Group','Date','tactics_problem_id']]\n",
    "group_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the rows which have incomplete dates because we can't use those kind of information anyway.\n",
    "#Also, for the rest of the dates, convert the format from YYYY.MM.DD to YYYY-MM-DD\n",
    "\n",
    "from datetime import date as dt\n",
    "r,c = group_date.shape\n",
    "\n",
    "for i in range(r):\n",
    "    st = str(group_date.iloc[i,1])\n",
    "    [year, month, date] = st.split('.')\n",
    "    \n",
    "    if year != '????' and month != '??' and date != '??':\n",
    "        group_date.iloc[i,1] = dt(int(year), int(month), int(date))\n",
    "    else:\n",
    "        group_date.iloc[i,1] = 'N/A'\n",
    "        \n",
    "group_date.drop(group_date[(group_date['Date'] == \"N/A\")].index, inplace = True)\n",
    "group_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the group_date dataframe by the Group and find the mean, standard deviation, min, max, and range of the dates.\n",
    "#Note that the Date is converted to integer by counting how many seconds from that date to present.\n",
    "\n",
    "group_date['Date_int'] = pd.to_datetime(group_date['Date']).astype(int)\n",
    "res = group_date.groupby('Group').agg(['mean', 'std', 'min', 'max'])\n",
    "res.columns = ['_'.join(c) for c in res.columns.values]\n",
    "\n",
    "res['Date_mean'] = pd.to_datetime(res['Date_int_mean'])\n",
    "res['Date_std'] = pd.to_timedelta(res['Date_int_std'])\n",
    "res['Date_min'] = pd.to_datetime(res['Date_int_min'])\n",
    "res['Date_max'] = pd.to_datetime(res['Date_int_max'])\n",
    "\n",
    "res = res[['Date_mean', 'Date_std', 'Date_min', 'Date_max']]\n",
    "res['Date_range'] = (res['Date_max'] - res['Date_min']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember that earlier we eliminated rows which have invalid/incomplete dates.\n",
    "#This procedure results in some groups having only 1 date. We denote these groups as invalid_groups below\n",
    "#Why invalid? Because when we execute res[res['Date_range'] == 0], we want to find groups which actually\n",
    "#have DUPLICATE (AND still VALID) dates. Having only 1 date obviously results in range = 0 too, so we want\n",
    "#to eliminate these groups. \n",
    "res[res['Date_range'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_groups = [13,42,44,51,59,105,107,122,142,197]\n",
    "#all_groups is a list of all groups WITHOUT REPETITION\n",
    "all_groups = list(np.unique(np.array(group_date['Group'])))\n",
    "#valid_groups are the desired groups, which are already filtered out so that it does not contain groups which\n",
    "#are listed in invalid_groups.\n",
    "valid_groups = []\n",
    "for group in all_groups:\n",
    "    if not(group in invalid_groups):\n",
    "        valid_groups.append(group)\n",
    "    else:\n",
    "        group_date = group_date[group_date['Group'] != group]\n",
    "\n",
    "res = pd.DataFrame(res, index = valid_groups)\n",
    "res.to_csv(\"statistics summary date creation.csv\")\n",
    "group_date.to_csv(\"group_date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res['Date_range'] == 0] #We want to try to investigate these groups later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = group_date.pivot(values = 'Date_int', index = None, columns = 'Group')\n",
    "#new\n",
    "boxplot = new.boxplot(column = valid_groups, figsize = (30,10))\n",
    "boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = group_date.plot.scatter(x='Group', y='Date',c='Blue', figsize = (20,10))\n",
    "ax1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Further Investigation for Groups with Member Size > Threshold </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a list of (length, group) tuples where length is size of the group and group is the corresponding group number.\n",
    "#We only take those which have length > threshold\n",
    "\n",
    "threshold = 1\n",
    "\n",
    "member_sorted = []\n",
    "for pair in dup_fen_full_to_ids:\n",
    "    length = len(dup_fen_full_to_ids[pair])\n",
    "    if length > threshold:\n",
    "        pos = dup_fen_full_list.index(pair)\n",
    "        group = temp.iloc[pos,1]\n",
    "        if group in valid_groups:\n",
    "            member_sorted.append((length,group))\n",
    "            \n",
    "member_sorted.sort(reverse = True)\n",
    "member_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the sorted groups in group_large_member_sorted, and the lengths in size_large_member_sorted\n",
    "group_large_member_sorted = []\n",
    "size_large_member_sorted = []\n",
    "for pair in member_sorted:\n",
    "    group_large_member_sorted.append(pair[1])\n",
    "    size_large_member_sorted.append(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_large_member = new.boxplot(column = group_large_member_sorted, figsize = (30,10))\n",
    "boxplot_large_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary that maps each group to its corresponding standard deviation of dates in seconds, and another\n",
    "#dictionary that maps each group to its corresponding standard deviation of dates in days\n",
    "group_list = list(res.index)\n",
    "group_to_std_seconds = {}\n",
    "group_to_std_days = {}\n",
    "for i in range(len(group_list)):\n",
    "    group_to_std_seconds[group_list[i]] = res.iloc[i,1].total_seconds()\n",
    "    group_to_std_days[group_list[i]] = res.iloc[i,1].days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_large_member = pd.DataFrame({'Group': group_large_member_sorted, 'Member Size': size_large_member_sorted})\n",
    "data_large_member['Std in Seconds'] = np.vectorize(group_to_std_seconds.get)(list(data_large_member['Group']))\n",
    "data_large_member['Std in Days'] = np.vectorize(group_to_std_days.get)(list(data_large_member['Group']))\n",
    "data_large_member.to_csv(\"data_large_member.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Group\", y=\"Member Size\", order = group_large_member_sorted, data=data_large_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Group\", y=\"Std in Seconds\", order = group_large_member_sorted, data=data_large_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Group\", y=\"Std in Days\", order = group_large_member_sorted, data=data_large_member)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Finding Pearson Correlation Between Standard Deviation and Cosine Distances (ONLY Successful Attempts)\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp[temp['Group'].isin(group_large_member_sorted)]\n",
    "problem_id_dup = df['tactics_problem_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.read_csv(\"/h/224/stevenhl/chess_dim_skill/src/word2vecf/vectors/size_alpha/vecs_0.16_265.txt\", sep=' ', header=None, skiprows=1)\n",
    "embedding = embedding.drop(len(embedding.columns)-1,axis=1)\n",
    "problem_id = list(embedding[0])\n",
    "embedding.rename(columns={0:'puzzle_id'}, inplace=True)\n",
    "embedding.set_index('puzzle_id',inplace=True)\n",
    "embedding = embedding.divide(np.linalg.norm(embedding, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_filtered = []\n",
    "for problem in problem_id_dup:\n",
    "    if problem in problem_id:\n",
    "        problem_filtered.append(problem)\n",
    "df = df[df['tactics_problem_id'].isin(problem_filtered)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics \n",
    "def generate_matrix(sampled_problems):\n",
    "    vecs = embedding.loc[sampled_problems]\n",
    "    cos_sim = sklearn.metrics.pairwise.cosine_similarity(vecs)\n",
    "    cos_dist = 1.0 - cos_sim\n",
    "    #Set really small numbers to 0\n",
    "    cos_dist=cos_dist.round(3)\n",
    "    #Create cosine distance table\n",
    "    table = pd.DataFrame(cos_dist, columns=sampled_problems,index=sampled_problems)\n",
    "    return table\n",
    "\n",
    "table = generate_matrix(problem_filtered)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_groups = list(np.unique(np.array(df['Group'])))\n",
    "group_to_id_dup = {}\n",
    "\n",
    "for group in all_groups:\n",
    "    group_to_id_dup[group] = []\n",
    "\n",
    "r,c = df.shape\n",
    "for i in range(r):\n",
    "    group_to_id_dup[df.iloc[i,1]].append(df.iloc[i,0])\n",
    "\n",
    "group_to_id_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_group_to_id_dup = {}\n",
    "new_all_groups = []\n",
    "for group in group_to_id_dup:\n",
    "    if len(group_to_id_dup[group]) > threshold:\n",
    "        new_group_to_id_dup[group] = group_to_id_dup[group]\n",
    "        new_all_groups.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cos_dist = []\n",
    "for group in new_group_to_id_dup:\n",
    "    problem_list = new_group_to_id_dup[group]\n",
    "    cos_dist = 0\n",
    "    count = 0\n",
    "    for i in range(len(problem_list)):\n",
    "        for j in range(i+1,len(problem_list)):\n",
    "            cos_dist += table.loc[problem_list[i], problem_list[j]]\n",
    "            count += 1\n",
    "    avg_cos_dist.append(cos_dist/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_group_to_id_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_list = []\n",
    "for group in new_all_groups:\n",
    "    if group_to_std_days[group] == 0: print(group)\n",
    "    std_list.append(group_to_std_days[group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(std_list,avg_cos_dist)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Finding Pearson Correlation Between Dates Difference and Cosine Distances (ONLY Successful Attempts) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_date = df[['tactics_problem_id','Date']]\n",
    "\n",
    "id_to_date = {}\n",
    "r,c = id_date.shape\n",
    "for i in range(r):\n",
    "    id_to_date[id_date.iloc[i,0]] = id_date.iloc[i,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def diff_days(date1,date2):\n",
    "    date1 = datetime.datetime.strptime(date1, \" %Y.%m.%d\").date()\n",
    "    date2 = datetime.datetime.strptime(date2, \" %Y.%m.%d\").date()\n",
    "    return abs((date1 - date2).days)\n",
    "\n",
    "def check(date1,date2):\n",
    "    [y1,m1,d1] = date1.split(\".\")\n",
    "    [y2,m2,d2] = date2.split(\".\")\n",
    "    if y1 != '????' and m1 != '??' and d1 != '??' and y2 != '????' and m2 != '??' and d2 != '??':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_date_avg = []\n",
    "cos_dist_avg = []\n",
    "diff_date_complete = []\n",
    "cos_dist_complete = []\n",
    "for group in new_group_to_id_dup:\n",
    "    problem_list = new_group_to_id_dup[group]\n",
    "    diff = 0\n",
    "    count = 0\n",
    "    cos_dist = 0\n",
    "    for i in range(len(problem_list)):\n",
    "        for j in range(i+1, len(problem_list)):\n",
    "            if check(id_to_date[problem_list[i]], id_to_date[problem_list[j]]) == True:\n",
    "                diff += diff_days(id_to_date[problem_list[i]], id_to_date[problem_list[j]])\n",
    "                cos_dist += table.loc[problem_list[i], problem_list[j]]\n",
    "                diff_date_complete.append(diff_days(id_to_date[problem_list[i]], id_to_date[problem_list[j]]))\n",
    "                cos_dist_complete.append(table.loc[problem_list[i], problem_list[j]])\n",
    "                count += 1\n",
    "    diff_date_avg.append(diff/count)\n",
    "    cos_dist_avg.append(cos_dist/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_date_avg,cos_dist_avg)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_date_complete,cos_dist_complete)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Finding Pearson Correlation Between Rating and Cosine Distances (ONLY Successful Attempts) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp[['tactics_problem_id','Group','rating']]\n",
    "df = df[temp['Group'].isin(group_large_member_sorted)]\n",
    "\n",
    "id_to_rating = {}\n",
    "r,c = df.shape\n",
    "for i in range(r):\n",
    "    id_to_rating[df.iloc[i,0]] = df.iloc[i,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_rating_avg = []\n",
    "cos_dist_avg = []\n",
    "diff_rating_complete = []\n",
    "cos_dist_complete = []\n",
    "for group in new_group_to_id_dup:\n",
    "    problem_list = new_group_to_id_dup[group]\n",
    "    diff = 0\n",
    "    count = 0\n",
    "    cos_dist = 0\n",
    "    for i in range(len(problem_list)):\n",
    "        for j in range(i+1, len(problem_list)):\n",
    "            diff += abs(id_to_rating[problem_list[i]]-id_to_rating[problem_list[j]])\n",
    "            cos_dist += table.loc[problem_list[i], problem_list[j]]\n",
    "            diff_rating_complete.append(abs(id_to_rating[problem_list[i]]-id_to_rating[problem_list[j]]))\n",
    "            cos_dist_complete.append(table.loc[problem_list[i], problem_list[j]])\n",
    "            count += 1\n",
    "    diff_rating_avg.append(diff/count)\n",
    "    cos_dist_avg.append(cos_dist/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_rating_avg,cos_dist_avg)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_rating_complete,cos_dist_complete)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Finding Pearson Correlation Between Rd and Cosine Distances (ONLY Successful Attempts) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp[['tactics_problem_id','Group','rd']]\n",
    "df = df[temp['Group'].isin(group_large_member_sorted)]\n",
    "\n",
    "id_to_rd= {}\n",
    "r,c = df.shape\n",
    "for i in range(r):\n",
    "    id_to_rd[df.iloc[i,0]] = df.iloc[i,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_rd_avg = []\n",
    "cos_dist_avg = []\n",
    "diff_rd_complete = []\n",
    "cos_dist_complete = []\n",
    "for group in new_group_to_id_dup:\n",
    "    problem_list = new_group_to_id_dup[group]\n",
    "    diff = 0\n",
    "    count = 0\n",
    "    cos_dist = 0\n",
    "    for i in range(len(problem_list)):\n",
    "        for j in range(i+1, len(problem_list)):\n",
    "            diff += abs(id_to_rd[problem_list[i]]-id_to_rd[problem_list[j]])\n",
    "            cos_dist += table.loc[problem_list[i], problem_list[j]]\n",
    "            diff_rd_complete.append(abs(id_to_rd[problem_list[i]]-id_to_rd[problem_list[j]]))\n",
    "            cos_dist_complete.append(table.loc[problem_list[i], problem_list[j]])\n",
    "            count += 1\n",
    "    diff_rd_avg.append(diff/count)\n",
    "    cos_dist_avg.append(cos_dist/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_rd_avg,cos_dist_avg)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_rd_complete,cos_dist_complete)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Finding Pearson Correlation Between Attempt Count and Cosine Distances (ONLY Successful Attempts) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp[['tactics_problem_id','Group','attempt_count']]\n",
    "df = df[temp['Group'].isin(group_large_member_sorted)]\n",
    "\n",
    "id_to_attempt= {}\n",
    "r,c = df.shape\n",
    "for i in range(r):\n",
    "    id_to_attempt[df.iloc[i,0]] = df.iloc[i,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_attempt_avg = []\n",
    "cos_dist_avg = []\n",
    "diff_attempt_complete = []\n",
    "cos_dist_complete = []\n",
    "for group in new_group_to_id_dup:\n",
    "    problem_list = new_group_to_id_dup[group]\n",
    "    diff = 0\n",
    "    count = 0\n",
    "    cos_dist = 0\n",
    "    for i in range(len(problem_list)):\n",
    "        for j in range(i+1, len(problem_list)):\n",
    "            diff += abs(id_to_attempt[problem_list[i]]-id_to_attempt[problem_list[j]])\n",
    "            cos_dist += table.loc[problem_list[i], problem_list[j]]\n",
    "            diff_attempt_complete.append(abs(id_to_attempt[problem_list[i]]-id_to_attempt[problem_list[j]]))\n",
    "            cos_dist_complete.append(table.loc[problem_list[i], problem_list[j]])\n",
    "            count += 1\n",
    "    diff_attempt_avg.append(diff/count)\n",
    "    cos_dist_avg.append(cos_dist/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_attempt_avg,cos_dist_avg)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_attempt_complete,cos_dist_complete)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Finding Pearson Correlation Between Time and Cosine Distances (ONLY Successful Attempts) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp[['tactics_problem_id','Group','average_seconds']]\n",
    "df = df[temp['Group'].isin(group_large_member_sorted)]\n",
    "\n",
    "id_to_seconds= {}\n",
    "r,c = df.shape\n",
    "for i in range(r):\n",
    "    id_to_seconds[df.iloc[i,0]] = df.iloc[i,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_seconds_avg = []\n",
    "cos_dist_avg = []\n",
    "diff_seconds_complete = []\n",
    "cos_dist_complete = []\n",
    "for group in new_group_to_id_dup:\n",
    "    problem_list = new_group_to_id_dup[group]\n",
    "    diff = 0\n",
    "    count = 0\n",
    "    cos_dist = 0\n",
    "    for i in range(len(problem_list)):\n",
    "        for j in range(i+1, len(problem_list)):\n",
    "            diff += abs(id_to_seconds[problem_list[i]]-id_to_seconds[problem_list[j]])\n",
    "            cos_dist += table.loc[problem_list[i], problem_list[j]]\n",
    "            diff_seconds_complete.append(abs(id_to_seconds[problem_list[i]]-id_to_seconds[problem_list[j]]))\n",
    "            cos_dist_complete.append(table.loc[problem_list[i], problem_list[j]])\n",
    "            count += 1\n",
    "    diff_seconds_avg.append(diff/count)\n",
    "    cos_dist_avg.append(cos_dist/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_seconds_avg,cos_dist_avg)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(diff_seconds_complete,cos_dist_complete)\n",
    "corr[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Finding Pearson Correlation Between Standard Deviation and Cosine Distances (ALL attempts) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "chess_data = pd.read_csv('/w/225/1/chess/tactics/glicko_user_tactics_problem.csv_00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_data = chess_data[chess_data['tactics_problem_id'].isin(problem_id_dup)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vocabulary file for word2vecf\n",
    "vocab_dat = chess_data.groupby('tactics_problem_id')['user_hash'].nunique().to_frame()\n",
    "\n",
    "#Get the count of puzzles attempted per each user, ie create the context file for word2vecf\n",
    "context_dat = chess_data.groupby('user_hash')['tactics_problem_id'].nunique().to_frame()\n",
    "\n",
    "#Write to CSV the training, vocab and context data\n",
    "#Convert indexes to columns\n",
    "vocab_dat.reset_index(level=0, inplace=True)\n",
    "vocab_dat.to_csv('vocab_dat_new',\n",
    "                 sep=' ',\n",
    "                 index=False,\n",
    "                 header=False,\n",
    "                 encoding='utf-8')\n",
    "\n",
    "context_dat.reset_index(level=0,inplace=True)\n",
    "context_dat.to_csv('context_dat_new',\n",
    "                 sep=' ',\n",
    "                 index=False,\n",
    "                 header=False,\n",
    "                 encoding='utf-8')\n",
    "\n",
    "#Filter and reorder columns\n",
    "chess_data = chess_data[['tactics_problem_id','user_hash']]\n",
    "chess_data.to_csv('chess_data_new',\n",
    "                 sep=' ',\n",
    "                 index=False,\n",
    "                 header=False,\n",
    "                 encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dat = pd.read_csv(\"context_dat_new\")\n",
    "vocab_dat = pd.read_csv(\"vocab_dat_new\")\n",
    "chess_data = pd.read_csv(\"chess_data_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bunch of random pairs of \n",
    "pairs_to_sample = 1000000 # will be less b.c. duplicates\n",
    "vocab_dat = vocab_dat.set_index('tactics_problem_id')\n",
    "pairs = np.random.choice(vocab_dat.index, (pairs_to_sample,2))\n",
    "pairs = pairs[pairs[:, 0] != pairs[:, 1]]\n",
    "print(pairs[:5])\n",
    "print(pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join pairs with their user counts for each puzzle\n",
    "pmi_table = pd.DataFrame(data=pairs,columns=[\"puz_a\",\"puz_b\"])\n",
    "pmi_table = pd.merge(pmi_table,vocab_dat,how='left',left_on='puz_a',right_on='tactics_problem_id')\n",
    "#mi_table.columns = ['puz_a','puz_b','users_a',]\n",
    "pmi_table = pd.merge(pmi_table,vocab_dat,how='left',left_on='puz_b',right_on='tactics_problem_id')\n",
    "pmi_table.columns = ['puz_a','puz_b','users_a','users_b']\n",
    "pmi_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort for speeding up finding counts of shared users later\n",
    "chess_data.sort_values(by=\"tactics_problem_id\",inplace=True)\n",
    "chess_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "#Calculate number of users that have completed both puzzles for each pair\n",
    "import time\n",
    "def get_shared_user(row):\n",
    "    #temp = chess_data[(chess_data['tactics_problem_id'] == row['puz_a']) | (chess_data['tactics_problem_id'] == row['puz_b'])]\n",
    "    #return(len(temp[temp.duplicated(['user_hash'])]))\n",
    "    #Option B, slightly slower\n",
    "    #user_a = chess_data[chess_data['tactics_problem_id'] == row['puz_a']]\n",
    "    #user_b = chess_data[chess_data['tactics_problem_id'] == row['puz_b']]\n",
    "    #return(len(pd.merge(user_a,user_b,how='inner',on='user_hash')))\n",
    "    #Option C, requires sorted list, fastest\n",
    "    a_start = np.searchsorted(chess_data['tactics_problem_id'],row['puz_a'])\n",
    "    #print(a_first_index)\n",
    "    b_start = np.searchsorted(chess_data['tactics_problem_id'],row['puz_b'])\n",
    "    user_a = chess_data.iloc[a_start:a_start+row['users_a']]\n",
    "    user_b = chess_data.iloc[b_start:b_start+row['users_b']]\n",
    "    return(len(pd.merge(user_a,user_b,how='inner',on='user_hash')))\n",
    "\n",
    "start = time.time()\n",
    "pmi_table['users_a_b'] = pmi_table.progress_apply(lambda row: get_shared_user(row),axis=1)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pointwise mutual information for each pair\n",
    "def calc_pmi(row,vocab_len):\n",
    "    if(row['users_a_b'] == 0): return 0\n",
    "    return np.log(row['users_a_b']) + vocab_len - np.log(row['users_a']) - np.log(row['users_b'])\n",
    "\n",
    "vocab_len = np.log(len(vocab_dat.index))\n",
    "pmi_table['pmi'] = pmi_table.apply(lambda row: calc_pmi(row,vocab_len),axis=1)\n",
    "#Filter out negative values to get Postivive PMI\n",
    "#pmi_table = pmi_table[pmi_table['pmi'] > 0]\n",
    "pmi_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi = np.array(pmi_table['pmi'])\n",
    "for val in pmi:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save PPMI\n",
    "pmi_table.to_csv('ppmi_new', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Date, Event, Site </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the groups that we need to investigate further based on the previous result\n",
    "groups_to_investigate = [7,145,163,196,206] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = temp[['Group','Date','Event','Site']]\n",
    "new_data = new_data[new_data['Group'].isin(groups_to_investigate)]\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = temp[temp['Group'].isin([7,145,163])]\n",
    "x = x[['tactics_problem_id', 'Group', 'tags']]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> MRR for Exact Duplicates </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = pd.read_csv('embeddings/250_0.16_vectors.tsv', sep=' ', header=None, skiprows=1)\n",
    "temp = pd.read_csv(\"/h/224/stevenhl/chess_dim_skill/src/word2vecf/vectors/size_alpha/vecs_0.16_265.txt\", sep=' ', header=None, skiprows=1)\n",
    "temp = temp.drop(len(temp.columns)-1,axis=1)\n",
    "problem_id = list(temp[0])\n",
    "temp.rename(columns={0:'puzzle_id'}, inplace=True)\n",
    "puzzle_id = list(temp['puzzle_id'])\n",
    "temp.set_index('puzzle_id',inplace=True)\n",
    "temp = temp.divide(np.linalg.norm(temp, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample tags and generate distance matrix of those tags\n",
    "import sklearn.metrics\n",
    "def generate_matrix(sampled_problems):\n",
    "    vecs = temp.loc[sampled_problems]\n",
    "    cos_sim = sklearn.metrics.pairwise.cosine_similarity(vecs)\n",
    "    cos_dist = 1.0 - cos_sim\n",
    "    #Set really small numbers to 0\n",
    "    cos_dist=cos_dist.round(3)\n",
    "    #Create cosine distance table\n",
    "    table = pd.DataFrame(cos_dist, columns=sampled_problems,index=sampled_problems)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = generate_matrix(puzzle_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new table which is a modified version of table. The difference lies in the columns, where each column is\n",
    "#the IDs in duplicate_data_id above. Basically, new_table is a \"subset\" of table. Note that the rows are still exactly\n",
    "#the same as above since otherwise, the ranking won't be accurate.\n",
    "\n",
    "new_table = pd.DataFrame(table, columns = target_problems)\n",
    "new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = new_table.shape\n",
    "\n",
    "#temp is a matrix which is similar to new_table, except that each entry is a tuple of distance and its corresponding\n",
    "#problem ID. The ordering for the tuple is (distance, problem_ID).\n",
    "temp = np.empty([r,c], dtype = object)\n",
    "\n",
    "for i in tqdm(range(r)):\n",
    "    for j in range(c):\n",
    "        temp[i][j] = (new_table.iloc[i,j], problem_id[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = np.sort(temp, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = new_table.shape\n",
    "reciprocal_rank_fen_full = []\n",
    "rank_fen_full = []\n",
    "\n",
    "for i in tqdm(range(len(duplicate_fen_full_id_to_id))):\n",
    "    current_id = duplicate_fen_full_id_to_id[i][0] #get the ID on the left as the current\n",
    "    target_id = duplicate_fen_full_id_to_id[i][1] #get the ID on the right as the target\n",
    "    pos = target_problems.index(current_id) #find the column position\n",
    "    if temp1[0][pos][0] != 'nan': #if a column consists of nan, then it's invalid\n",
    "        for j in range(r):\n",
    "            if target_id == temp1[j][pos][1]:\n",
    "                rank_fen_full.append(j+1)\n",
    "                reciprocal_rank_fen_full.append(1./(j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reciprocal_rank_fen_full = sum(reciprocal_rank_fen_full) / len(reciprocal_rank_fen_full)\n",
    "mean_reciprocal_rank_fen_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(reciprocal_rank_fen_full, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rank_fen_full, bins = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
